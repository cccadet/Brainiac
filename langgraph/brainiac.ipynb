{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "from typing import Literal\n",
    "\n",
    "search = GoogleSerperAPIWrapper(gl='br', hl='pt-BR', k=3)\n",
    "\n",
    "\n",
    "@tool\n",
    "def serper_tool(question: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Useful for when you need to ask with search\"\"\"\n",
    "    return search.run(question)\n",
    "    \n",
    "\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Utilities\n",
    "\n",
    "Define a helper function that we will use to create the nodes in the graph - it takes care of converting the agent response to a human message. This is important because that is how we will add it the global state of the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Supervisor\n",
    "\n",
    "It will use function calling to choose the next worker node OR finish processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "members = [\"Behavior Planner\", \"Decision Maker\" , \"Social Behavior Modulator\", \"Complex Thought Planner\", \"Personality Express Planner\"]\n",
    "system_prompt = (\n",
    "    \"You are the Prefrontal Manager, a supervisor responsible for overseeing and coordinating \"\n",
    "    \"the actions of all agents within the system. Your role is to ensure that each agent's tasks \"\n",
    "    \"are aligned with the overall goals, utilizing the advanced capabilities of the prefrontal \"\n",
    "    \"cortex to optimize decision-making and behavior modulation.\"\n",
    "\n",
    "    \"You must manage a conversation between the following workers: {members}. Given the following \"\n",
    "    \"user request, reason about the appropriate actions and distribute tasks among the agents, ensuring \"\n",
    "    \"that each contributes effectively to the global objective. Create a strategic plan that outlines \"\n",
    "    \"how the following agents will operate: \"\n",
    "\n",
    "    \"Behavior Planner (manages complex and adaptive behaviors) \"\n",
    "    \"Decision Maker (handles decision-making processes) \"\n",
    "    \"Social Behavior Modulator (modulates social behaviors) \"\n",
    "    \"Complex Thought Planner (plans complex reasoning tasks) \"\n",
    "    \"Personality Express Planner (plans personality-driven actions) \"\n",
    "\n",
    "    \"Each agent will perform its task and respond with results and status. Ensure that their actions \"\n",
    "    \"are cohesive and aligned with the systemâ€™s goals. Once all tasks are complete, respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "class routeResponse(BaseModel):\n",
    "    next: Literal[*options]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.with_structured_output(routeResponse)\n",
    "    )\n",
    "    return supervisor_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph\n",
    "\n",
    "We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x749369b74a40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent = create_react_agent(llm, tools=[serper_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
    "code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", code_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Finally, add entrypoint\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the team\n",
    "\n",
    "With the graph created, we can now invoke it and see how it performs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Coder': {'messages': [HumanMessage(content=\"The code `print('Hello, World!')` has been executed, and it printed: **Hello, World!** to the terminal.\", additional_kwargs={}, response_metadata={}, name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Researcher'}}\n",
      "----\n",
      "{'Researcher': {'messages': [HumanMessage(content='### Research Report on Pikas\\n\\n#### Introduction\\nPikas, small, herbivorous mammals belonging to the family Ochotonidae, are closely related to rabbits and hares. They inhabit mountainous regions in North America, Asia, and Europe. Pikas are notable for their unique adaptations to cold environments, their vocalizations, and their role in the ecosystem.\\n\\n#### Physical Characteristics\\nPikas are small, typically measuring about 20 to 30 centimeters in length and weighing between 120 to 500 grams. They have rounded bodies, short limbs, and no visible tail. Their fur is thick and varies in color from brown to gray, which helps them blend into rocky environments. Pikas have large, rounded ears that aid in hearing predators and regulating body temperature.\\n\\n#### Habitat and Distribution\\nPikas are primarily found in alpine and subalpine regions, often at elevations between 2,000 to 4,000 meters. They prefer rocky slopes and talus fields, which provide shelter from predators and harsh weather conditions. The two main species of pikas found in North America are the American pika (Ochotona princeps) and the tundra pika (Ochotona hyperborea).\\n\\n#### Behavior and Diet\\nPikas are diurnal and are known for their distinctive vocalizations, which serve as communication among individuals. They are herbivorous and primarily feed on grasses, herbs, and other vegetation found in their mountainous habitats. Pikas exhibit a behavior known as \"haypiling,\" where they collect and store food for winter months, as they do not hibernate.\\n\\n#### Conservation Status\\nPikas are considered sensitive to climate change, particularly due to their reliance on cold environments. Rising temperatures and habitat loss threaten their populations. Some species are classified as near threatened or vulnerable by conservation organizations. Monitoring and research are critical to understanding the impacts of climate change on pika populations and ecosystems.\\n\\n#### Conclusion\\nPikas are fascinating mammals with unique adaptations to their cold habitats. Their role in the ecosystem is vital, and ongoing conservation efforts are necessary to ensure their survival in the face of environmental changes. Further research is essential to grasp the full impact of climate changes on their behavior, distribution, and population dynamics.\\n\\n#### References\\n- Smith, A. T., & Weston, M. L. (1990). \"Pikas and Climate Change: a Review of the Evidence.\" *Ecological Applications*.\\n- Beever, E. A., et al. (2010). \"Changes in Pika Populations in Response to Climate Change.\" *Ecology and Evolution*.\\n- Grayson, D. K. (2005). \"The Late Quaternary Extinction of Pikas in North America.\" *Quaternary Science Reviews*.', additional_kwargs={}, response_metadata={}, name='Researcher')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n",
      "{'Coder': {'messages': [HumanMessage(content='### Research Report on Pikas\\n\\n#### Introduction\\nPikas, small, herbivorous mammals belonging to the family Ochotonidae, are closely related to rabbits and hares. They inhabit mountainous regions in North America, Asia, and Europe. Pikas are notable for their unique adaptations to cold environments, their vocalizations, and their role in the ecosystem.\\n\\n#### Physical Characteristics\\nPikas are small, typically measuring about 20 to 30 centimeters in length and weighing between 120 to 500 grams. They have rounded bodies, short limbs, and no visible tail. Their fur is thick and varies in color from brown to gray, which helps them blend into rocky environments. Pikas have large, rounded ears that aid in hearing predators and regulating body temperature.\\n\\n#### Habitat and Distribution\\nPikas are primarily found in alpine and subalpine regions, often at elevations between 2,000 to 4,000 meters. They prefer rocky slopes and talus fields, which provide shelter from predators and harsh weather conditions. The two main species of pikas found in North America are the American pika (Ochotona princeps) and the tundra pika (Ochotona hyperborea).\\n\\n#### Behavior and Diet\\nPikas are diurnal and are known for their distinctive vocalizations, which serve as communication among individuals. They are herbivorous and primarily feed on grasses, herbs, and other vegetation found in their mountainous habitats. Pikas exhibit a behavior known as \"haypiling,\" where they collect and store food for winter months, as they do not hibernate.\\n\\n#### Conservation Status\\nPikas are considered sensitive to climate change, particularly due to their reliance on cold environments. Rising temperatures and habitat loss threaten their populations. Some species are classified as near threatened or vulnerable by conservation organizations. Monitoring and research are critical to understanding the impacts of climate change on pika populations and ecosystems.\\n\\n#### Conclusion\\nPikas are fascinating mammals with unique adaptations to their cold habitats. Their role in the ecosystem is vital, and ongoing conservation efforts are necessary to ensure their survival in the face of environmental changes. Further research is essential to grasp the full impact of climate changes on their behavior, distribution, and population dynamics.\\n\\n#### References\\n- Smith, A. T., & Weston, M. L. (1990). \"Pikas and Climate Change: a Review of the Evidence.\" *Ecological Applications*.\\n- Beever, E. A., et al. (2010). \"Changes in Pika Populations in Response to Climate Change.\" *Ecology and Evolution*.\\n- Grayson, D. K. (2005). \"The Late Quaternary Extinction of Pikas in North America.\" *Quaternary Science Reviews*.', additional_kwargs={}, response_metadata={}, name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Write a brief research report on pikas.\")]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('user', \"What's the weather in NYC?\")\n",
      "('user', \"What's the weather in NYC?\")\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Researcher\n",
      "\n",
      "It seems that the information retrieved did not include the current weather for New York City. Would you like me to check again or provide information on something else?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Researcher\n",
      "\n",
      "It seems that the information retrieved did not include the current weather for New York City. Would you like me to check again or provide information on something else?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Researcher\n",
      "\n",
      "It seems I wasn't able to retrieve the current weather information for New York City. Would you like me to assist you with something else?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Researcher\n",
      "\n",
      "It seems I wasn't able to retrieve the current weather information for New York City. Would you like me to assist you with something else?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Coder\n",
      "\n",
      "I can't check the current weather for you at the moment, but I can help you with other information or tasks if you need! Just let me know what you're interested in.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Coder\n",
      "\n",
      "I can't check the current weather for you at the moment, but I can help you with other information or tasks if you need! Just let me know what you're interested in.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
